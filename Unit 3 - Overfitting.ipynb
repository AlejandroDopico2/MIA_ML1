{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f8712e9",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "One of the most important issues in Machine Learning is to understand the difference between training and testing errors. The training process will allow the ANN parameters (weights and bias) to be modified to minimise the loss in the **set of patterns used for training**.  If this set of patterns is sufficiently representative, then the ANN will perform well in its working environment, i.e. with new patterns that are not in the training set.  On the other hand, if this set of patterns has not been well chosen, the ANN could not work well with new patterns.  The only way to know this is precisely to evaluate the ANN with new patterns and calculate the error with these **test patterns**.  Since the test set is available from the beginning, this evaluation with the test set is done at the same time as with the training set (but only the training set is used for training). \n",
    "\n",
    "Therefore, to assess how well trained an ANN is, this can be done by evaluating the test set, not the training set.  Usually there is only one set of data, so it is usually divided into two subsets: training and test. There are several alternatives to perform this division; in these tutorials, two different ones will be performed: \n",
    "\n",
    "* **Hold Out**. In this method, an experiment is performed with a simple partition of the data set, leaving a certain percentage for testing, usually between 20% and 30%. \n",
    "\n",
    "* **Cross-validation**.  Here, the data set is divided into $k$ disjoint subsets in order to perform the corresponding $k$ experiments.  In the k-th experiment, the $k$-th set is separated for testing, and trained with the remaining $k-1$.  A usual value of $k$ is 10, so you have a 10-fold crossvalidation.\n",
    "\n",
    "More specifically, in this session we will focus on the first one, leaving the second one for a later one.  The hold out technique allows us to split the data set into two subsets: training and test.  The training subset allows to adjust the parameters of the model (connection weights and bias), while the test subset allows to check (test) the behavious of a trained ANN (or, any kind of model that is being used, such as a decision tree) on new patterns, not present in the training set.\n",
    "\n",
    "![Normal training in the left, and an overfitting curve on the right](./img/Overfitting.png)\n",
    "\n",
    "Given a well-chosen (and therefore sufficiently representative) training set, the training process could be similar to the figure on the left, with both training and test errors decreasing with each cycle. However, in general, it cannot be ensured that the training set is as noise-free and well-chosen as desirable, so that **overtraining** or **overfitting** often occurs during training.  This can be clearly seen in the figure on the right, where both errors are decreasing until a point is reached where the test error starts to increase.   From that point on, the system starts to overfit. \n",
    "\n",
    "To avoid overfitting, in ANNs there are a series of techniques known as regularisation techniques, each of which is very different in nature, but all of which aim to avoid overtraining.  Although these techniques are dealt with in much greater depth in other subjects in the programme, e.g. Deep Learning I, in these tutorials we are going to study one of the most well-known techniques, seen in the theory session, called Early Stopping. \n",
    "\n",
    "In this technique, instead of dividing the initial set of samples into 2 subsets, we create 3 subsets: training, validation and test.  The training and test sets perform the functions described above. The purpose of the validation set is to avoid overfitting by controlling the training process, but it is not used to modify weights or bias.  There are several ways to control the training process, here we will use two of them together:\n",
    "\n",
    "* A new criteria for stopping the training process.   With the definition of a new parameter: the maximum number of consecutive cycles without improving the *loss* in the validation set.  If these cycles elapse without improving the best validation *loss* achieved so far, the training is stopped.  As the validation set is not used to modify weights or bias, it gives an estimate of what the test error could be.  Therefore, when the validation error increases, it increases the error in patterns that it has not seen, so it is estimated that the test error would increase, and the network would be overtraining.\n",
    "\n",
    "\n",
    "* Once the training process has stopped, the validation set determines which network is returned.  During all the training cycles, the loss is calculated in the training set, which is necessary to modify weights and bias, but also in the validation set.   Once the training is stopped, the ANN is returned corresponding not to the last cycle, but to the cycle with the lowest error in the validation set.  This is done because it is estimated that this ANN is the one that will give the lowest error in the test set, i.e. the one that will behave better with new patterns and, consequently, the one that will generalise better. \n",
    "\n",
    "Once the training process has stopped and an ANN has been returned, the test set can be evaluated to assess how well the ANN has been trained. \n",
    "\n",
    "> **Important**: The errors (or accuracies, or any metric used) in the training and validation sets are interesting, but the one that really matters is the test set. \n",
    "\n",
    "However, most of the time you have all 3 sets (training, validation and test) from the beginning, so as you evaluate the training and validation sets, you also evaluate the test set, which is useful for plotting graphs. However, these values in the test set cannot be used for decision making. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3c258c",
   "metadata": {},
   "source": [
    "### Question\n",
    "If what we want to achieve is an ANN with the lowest possible test error, why not choose the ANN corresponding to the cycle with the lowest test error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aea394",
   "metadata": {},
   "source": [
    "`Answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa354eee",
   "metadata": {},
   "source": [
    "Therefore, in the training loop it is necessary to keep a reference to the best validation error found so far.  It is also necessary to keep a copy of the ANN that has given that validation error.  Both elements should be updated every cycle when the validation error improves. It is also necessary to keep a counter of how many consecutive cycles the validation error has not improved.\n",
    "\n",
    "As can be seen, the test set cannot have any involvement in the training process. This is the right thing to do, since an ANN with completely new patterns is being evaluated. However, from a very strict point of view, there is one part in this whole process where the test set is having a small influence: in the normalisation of the data.\n",
    "\n",
    "Usually the starting point is a dataset that is first normalised and then split into training/test.  If this is done in this way, the patterns used for testing have some influence on the calculation of the normalisation parameter values, which would not be correct. However, the impact they usually have on these parameters and in general on the performance of the ANN is so low, that this is usually not taken into account and all patterns are normalised together.  In any case, the really correct pipeline to do would be divide the data into train test, to calculate the values of the normalisation parameters only with the training set, and the applied that parameters of normalization to the test prior to run the usual training process. Addtionally, if there is a validation data, it can be used on the normalization, too.\n",
    "\n",
    "For this assignment, you are asked to perform two sets of experiments, one without using validation and the other using a validation set.  More specifically, the requirements are: \n",
    "\n",
    "1. Develop a function called `holdOut` that given two parameters, `N` (equal to the number of patterns) and `P` (value between 0 and 1, indicating the percentage of patterns that will be separated for the test set), returns a tuple with two vectors with the indices of the patterns that will be used for training and testing.  The sum of lengths of both vectors has to be equal to N, and these two vectors have to be disjoint.\n",
    "\n",
    "  This function can be done in a very simple way using the function `randperm` (to use it, load the module with `using Random`).\n",
    "  \n",
    "  From this function, splitting a database into two subsets is done immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab11d8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "holdOut (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random;\n",
    "\n",
    "function holdOut(N::Int, P::Real)\n",
    "\n",
    "    @assert ((P >= 0.) & (P <= 1.))\n",
    "\n",
    "    indexes = randperm(N)\n",
    "\n",
    "    test_size = Int(ceil(N*P))\n",
    "\n",
    "    test_indexes = indexes[1:test_size]\n",
    "    train_indexes = indexes[test_size+1:end]\n",
    "\n",
    "    @assert length(train_indexes) + length(test_indexes) == N\n",
    "    return train_indexes, test_indexes\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3d6e5f",
   "metadata": {},
   "source": [
    "2. Develop another function called `holdOut`, the same as the previous one and, based on it, which takes 3 parameters: `N` (number of patterns), `Pval` (rate of patterns in the validation set) and `Ptest` (rate of patterns in the test set), and returns a tuple with 3 vectors, with the indices of the elements of the training, validation and test sets. The sum of the lengths of these 3 vectors has to be equal to `N`.\n",
    "\n",
    " To do this, simply make two calls to the previously developed `holdOut` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "691dc785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "holdOut (generic function with 2 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function holdOut(N::Int, Pval::Real, Ptest::Real) \n",
    "\n",
    "    @assert ((Pval>=0.) & (Pval<=1.))\n",
    "    @assert ((Ptest>=0.) & (Ptest<=1.))\n",
    "    @assert ((Pval+Ptest)<=1.)\n",
    "\n",
    "    train_val_indexes, test_indexes = holdOut(N, Ptest)\n",
    "\n",
    "    train_val_size = length(train_val_indexes)\n",
    "\n",
    "    train_indexes, val_indexes = holdOut(train_val_size, Pval * N / train_val_size)\n",
    "\n",
    "    train_indexes = train_val_indexes[train_indexes]\n",
    "    val_indexes = train_val_indexes[val_indexes]\n",
    "\n",
    "    @assert length(train_indexes) + length(val_indexes) + length(test_indexes) == N\n",
    "\n",
    "    return train_indexes, val_indexes, test_indexes\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca757d71",
   "metadata": {},
   "source": [
    "3. Modify the function for training the ANN developed in the previous session to accept the following *optional* parameters in addition to those defined in the previous practice:\n",
    "  - **Validation set**, of type `Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}}`, i.e. a tuple of 2 arrays: inputs and targets. Default value to empty arrays.\n",
    "  - **Test set**, of type `Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}}`, i.e. a tuple of 2 arrays: inputs and targets. Default value to empty arrays.\n",
    "  - **maxEpochsVal**, of type `Int`, which defines the number of cycles without improving the best validation loss found so far that have to elapse to stop training. In other words, it defines a new stop criterion. Default value should be set to 20.\n",
    "  \n",
    "  With these parameters, this function should implement the early stopping strategy, for which it is necessary to take care that the ANN returned at the end of the function must be: \n",
    "    - If a validation set has been given as a argument (i.e. if it is not empty), the ANN to be returned does not have to be the one being trained, but the one with the best validation error.\n",
    "    - If no validation set has been passed as a parameter (i.e. if it is empty), the ANN to be returned should be the one corresponding to the last training cycle.  Consequently, the new training function should work in the same way as the previous one in case no validation set is given.\n",
    "    \n",
    "  Additionally, from the trained ANN, this function should return a history of the loss values obtained in the training, validation and test sets in each cycle, as a vector each.\n",
    "  \n",
    "  - To add elements to the end of a vector, refer to the function `push!`\n",
    "  - It should be noted that the loss values obtained with the ANN with random weights, prior to training, are usually considered as cycle 0. \n",
    "  \n",
    "  As stated in the previous description, it will be necessary to store the best ANN achieved so far and update it in some iterations of the loop.  To do this, you cannot simply make an assignment to a new variable, since you would only assign the pointer, which would point to the same memory address with the ANN that would be modified in the next iteration.  To make a copy of an object in such a way that all objects (and data, as in this case weights and bias) it contains are also copied, recursively, you can use the function `deepcopy`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40252fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainClassANN (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function trainClassANN(topology::AbstractArray{<:Int,1},  \n",
    "            trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}}; \n",
    "            validationDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}}= \n",
    "                    (Array{eltype(trainingDataset[1]),2}(undef,0,0), falses(0,0)), \n",
    "            testDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,2}}= \n",
    "                    (Array{eltype(trainingDataset[1]),2}(undef,0,0), falses(0,0)), \n",
    "            transferFunctions::AbstractArray{<:Function,1}=fill(σ, length(topology)), \n",
    "            maxEpochs::Int=1000, minLoss::Real=0.0, learningRate::Real=0.01,  \n",
    "            maxEpochsVal::Int=20, showText::Bool=false) \n",
    "    \n",
    "    trainingInputs, trainingTargets = trainingDataset\n",
    "    testInputs, testTargets = testDataset\n",
    "\n",
    "    is_val_assigned = isassigned(validationDataset[1], 2)\n",
    "\n",
    "    @assert(size(trainingInputs, 1) == size(trainingTargets, 1))\n",
    "    @assert(size(testInputs, 1) == size(testTargets, 1))\n",
    "\n",
    "    if (is_val_assigned)\n",
    "        validationInputs, validationTargets = validationDataset\n",
    "\n",
    "        @assert(size(validationInputs, 1) == size(validationTargets, 1))\n",
    "        @assert(size(trainingInputs, 2) == size(validationInputs, 2) == size(testInputs, 2))\n",
    "        @assert(size(trainingTargets, 2) == size(validationTargets, 2) == size(testTargets, 2))\n",
    "    else\n",
    "        @assert(size(trainingInputs, 2) == size(testInputs, 2))\n",
    "        @assert(size(trainingTargets, 2) == size(testTargets, 2))\n",
    "    end\n",
    "\n",
    "    \n",
    "    ann = buildClassANN(size(inputs, 2), topology,\n",
    "                        size(targets, 2))\n",
    "\n",
    "    loss(model,x,y) = (size(y,1) == 1) ? Losses.binarycrossentropy(model(x),y) : Losses.crossentropy(model(x),y)\n",
    "\n",
    "    trainingLosses = Float32[]\n",
    "    validationLosses = Float32[]\n",
    "    testLosses = Float32[]\n",
    "    numEpoch = 0;\n",
    "\n",
    "    trainingLoss = loss(ann, trainingInputs', trainingTargets')\n",
    "    testLoss = loss(ann, testInputs', testTargets')\n",
    "    push!(trainingLosses, trainingLoss)\n",
    "    push!(testLosses, testLoss)\n",
    "\n",
    "\n",
    "    if (is_val_assigned)\n",
    "        validationLoss = loss(ann, validationInputs', validationTargets')\n",
    "        bestValidationLoss = validationLoss\n",
    "\n",
    "        if (showText)\n",
    "            println(\"Epoch \", numEpoch, \": Train Loss = \", trainingLoss, \", Validation Loss = \", validationLoss, \", Test Loss = \", testLoss)\n",
    "        end\n",
    "\n",
    "        push!(validationLosses, validationLoss)\n",
    "    elseif (!is_val_assigned && showText)\n",
    "\n",
    "        println(\"Epoch \", numEpoch, \": Train Loss = \", trainingLoss, \", Test Loss = \", testLoss)\n",
    "        \n",
    "    end\n",
    "    \n",
    "    opt_state = Flux.setup(Adam(learningRate), ann)\n",
    "\n",
    "    numEpochsVal = 0\n",
    "    bestAnn = deepcopy(ann)\n",
    "\n",
    "    while (numEpoch<maxEpochs) && (trainingLoss>minLoss) && (numEpochsVal<maxEpochsVal)\n",
    "        numEpoch += 1\n",
    "        Flux.train!(loss, ann, [(trainingInputs', trainingTargets')], opt_state)\n",
    "        trainingLoss = loss(ann, trainingInputs', trainingTargets')\n",
    "        testLoss = loss(ann, testInputs', testTargets')\n",
    "        push!(trainingLosses, trainingLoss)\n",
    "        push!(testLosses, testLoss)\n",
    "\n",
    "        if (is_val_assigned)\n",
    "            validationLoss = loss(ann, validationInputs', validationTargets')\n",
    "            push!(validationLosses, validationLoss)\n",
    "\n",
    "            if (validationLoss < bestValidationLoss)\n",
    "                bestValidationLoss = validationLoss\n",
    "                numEpochsVal = 0\n",
    "                bestAnn = deepcopy(ann)\n",
    "            else\n",
    "                numEpochsVal += 1\n",
    "            end\n",
    "            \n",
    "            if (showText)\n",
    "                println(\"Epoch \", numEpoch, \": Train Loss = \", trainingLoss, \", Validation Loss = \", validationLoss, \", Test Loss = \", testLoss)\n",
    "            elseif (!is_val_assigned && showText)\n",
    "                println(\"Epoch \", numEpoch, \": Train Loss = \", trainingLoss, \" Test Loss = \", testLoss)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    (is_val_assigned) ? (return bestAnn, trainingLosses, validationLosses, testLosses) : (return bestAnn, trainingLosses, testLosses)\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2308318",
   "metadata": {},
   "source": [
    "4. Since there were two versions of the ANN training function (the second accepted a vector as desired outputs), modify this function in the same way so that the targets are vectors instead of arrays for the two-class classification case. Note that in this case the types of the training, validation and test sets passed as parameters will be `Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,1}}` for all three and therefore it will be necessary to call the function `reshape` on all three. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66c1a7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainClassANN (generic function with 2 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function trainClassANN(topology::AbstractArray{<:Int,1},  \n",
    "        trainingDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,1}}; \n",
    "        validationDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,1}}= \n",
    "                    (Array{eltype(trainingDataset[1]),2}(undef,0,0), falses(0)), \n",
    "        testDataset::Tuple{AbstractArray{<:Real,2}, AbstractArray{Bool,1}}= \n",
    "                    (Array{eltype(trainingDataset[1]),2}(undef,0,0), falses(0)), \n",
    "        transferFunctions::AbstractArray{<:Function,1}=fill(σ, length(topology)),\n",
    "        maxEpochs::Int=1000, minLoss::Real=0.0, learningRate::Real=0.01,  \n",
    "        maxEpochsVal::Int=20, showText::Bool=false)\n",
    "\n",
    "    trainingInputs, trainingTargets = trainingDataset\n",
    "    testInputs, testTargets = testDataset\n",
    "\n",
    "    trainingTargets = reshape(trainingTargets, :, 1)\n",
    "    testTargets = reshape(testTargets, :, 1)\n",
    "\n",
    "    if isassigned(validationDataset[1], 2)\n",
    "        validationInputs, validationTargets = validationDataset\n",
    "        validationTargets = reshape(validationTargets, :, 1)\n",
    "\n",
    "        trainClassANN(topology, (trainingInputs, trainingTargets), (validationInputs, validationTargets), (testInputs, testTargets), transferFunctions, maxEpochs, minLoss, learningRate, maxEpochsVal, showText)\n",
    "    else\n",
    "        trainClassANN(topology, (trainingInputs, trainingTargets), (testInputs, testTargets), transferFunctions, maxEpochs, minLoss, learningRate, maxEpochsVal, showText)\n",
    "    end\n",
    "\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d1c1b6",
   "metadata": {},
   "source": [
    "Remember to integrate the code developed in previous sessions to validate the following pipeline:\n",
    "\n",
    "1. Load the database, having the patterns in rows and desired attributes and outputs in columns. \n",
    "\n",
    "2. Use the function `holdOut` to split the data set into training, validation and test with the desired percentages. These percentages can be equal to 0.  You will therefore have 6 matrices: inputs and targets in training, validation and test (some may be empty if the corresponding percentage is 0). \n",
    "\n",
    "3. Calculate the parameter values corresponding to the type of normalisation to be used with your data (maximum/minimum or mean/standard deviation for each attribute) from the training set only.\n",
    "  * This part does not ask to normalise the training set, but to calculate the normalisation values from the training set. \n",
    "4. With these parameters for normalisation calculated in the previous step, normalise training, validation and test sets.\n",
    "5. Train different architectures, and, for each one of them, draw graphs of the evolution of the training, validation and test loss values in the same graph, including cycle 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec741790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss = 1.1257397, Validation Loss = 1.18191, Test Loss = 1.1392792\n",
      "Epoch 1: Train Loss = 1.1093332, Validation Loss = 1.1554848, Test Loss = 1.1214775\n",
      "Epoch 2: Train Loss = 1.0961602, Validation Loss = 1.1333596, Test Loss = 1.1077902\n",
      "Epoch 3: Train Loss = 1.0859826, Validation Loss = 1.1148031, Test Loss = 1.0973995\n",
      "Epoch 4: Train Loss = 1.0785158, Validation Loss = 1.0996374, Test Loss = 1.0899854\n",
      "Epoch 5: Train Loss = 1.073342, Validation Loss = 1.0875475, Test Loss = 1.0850147\n",
      "Epoch 6: Train Loss = 1.0698757, Validation Loss = 1.0780667, Test Loss = 1.0817798\n",
      "Epoch 7: Train Loss = 1.0674129, Validation Loss = 1.07061, Test Loss = 1.0794969\n",
      "Epoch 8: Train Loss = 1.0652719, Validation Loss = 1.0645679, Test Loss = 1.0774478\n",
      "Epoch 9: Train Loss = 1.0629313, Validation Loss = 1.0594183, Test Loss = 1.0751036\n",
      "Epoch 10: Train Loss = 1.0600866, Validation Loss = 1.0547885, Test Loss = 1.0721632\n",
      "Epoch 11: Train Loss = 1.0566311, Validation Loss = 1.0504589, Test Loss = 1.0685265\n",
      "Epoch 12: Train Loss = 1.0526028, Validation Loss = 1.0463316, Test Loss = 1.0642366\n",
      "Epoch 13: Train Loss = 1.0481265, Validation Loss = 1.0423893, Test Loss = 1.0594238\n",
      "Epoch 14: Train Loss = 1.0433658, Validation Loss = 1.0386547, Test Loss = 1.054257\n",
      "Epoch 15: Train Loss = 1.038486, Validation Loss = 1.0351588, Test Loss = 1.0489087\n",
      "Epoch 16: Train Loss = 1.0336295, Validation Loss = 1.0319159, Test Loss = 1.0435288\n",
      "Epoch 17: Train Loss = 1.0288985, Validation Loss = 1.028907, Test Loss = 1.0382279\n",
      "Epoch 18: Train Loss = 1.0243475, Validation Loss = 1.0260713, Test Loss = 1.033069\n",
      "Epoch 19: Train Loss = 1.0199809, Validation Loss = 1.0233074, Test Loss = 1.0280654\n",
      "Epoch 20: Train Loss = 1.0157605, Validation Loss = 1.0204813, Test Loss = 1.023187\n",
      "Epoch 21: Train Loss = 1.0116161, Validation Loss = 1.0174426, Test Loss = 1.0183712\n",
      "Epoch 22: Train Loss = 1.0074632, Validation Loss = 1.0140445, Test Loss = 1.01354\n",
      "Epoch 23: Train Loss = 1.0032195, Validation Loss = 1.0101645, Test Loss = 1.0086166\n",
      "Epoch 24: Train Loss = 0.9988183, Validation Loss = 1.0057182, Test Loss = 1.0035397\n",
      "Epoch 25: Train Loss = 0.9942181, Validation Loss = 1.0006689, Test Loss = 0.9982715\n",
      "Epoch 26: Train Loss = 0.9894046, Validation Loss = 0.99502635, Test Loss = 0.992801\n",
      "Epoch 27: Train Loss = 0.9843873, Validation Loss = 0.9888405, Test Loss = 0.9871398\n",
      "Epoch 28: Train Loss = 0.9791928, Validation Loss = 0.9821907, Test Loss = 0.9813154\n",
      "Epoch 29: Train Loss = 0.9738559, Validation Loss = 0.9751728, Test Loss = 0.9753624\n",
      "Epoch 30: Train Loss = 0.9684121, Validation Loss = 0.9678873, Test Loss = 0.9693139\n",
      "Epoch 31: Train Loss = 0.9628894, Validation Loss = 0.9604292, Test Loss = 0.96319515\n",
      "Epoch 32: Train Loss = 0.95730525, Validation Loss = 0.9528801, Test Loss = 0.957019\n",
      "Epoch 33: Train Loss = 0.9516642, Validation Loss = 0.94530475, Test Loss = 0.95078504\n",
      "Epoch 34: Train Loss = 0.94596, Validation Loss = 0.9377495, Test Loss = 0.9444812\n",
      "Epoch 35: Train Loss = 0.94017726, Validation Loss = 0.93024284, Test Loss = 0.9380875\n",
      "Epoch 36: Train Loss = 0.93429744, Validation Loss = 0.92279935, Test Loss = 0.93158\n",
      "Epoch 37: Train Loss = 0.9283018, Validation Loss = 0.91542244, Test Loss = 0.92493683\n",
      "Epoch 38: Train Loss = 0.9221763, Validation Loss = 0.90810806, Test Loss = 0.9181407\n",
      "Epoch 39: Train Loss = 0.91591316, Validation Loss = 0.900848, Test Loss = 0.91118306\n",
      "Epoch 40: Train Loss = 0.90951216, Validation Loss = 0.89363045, Test Loss = 0.90406317\n",
      "Epoch 41: Train Loss = 0.90297884, Validation Loss = 0.8864418, Test Loss = 0.8967882\n",
      "Epoch 42: Train Loss = 0.8963241, Validation Loss = 0.8792662, Test Loss = 0.88937014\n",
      "Epoch 43: Train Loss = 0.8895603, Validation Loss = 0.8720845, Test Loss = 0.8818244\n",
      "Epoch 44: Train Loss = 0.8826996, Validation Loss = 0.8648746, Test Loss = 0.8741663\n",
      "Epoch 45: Train Loss = 0.87575233, Validation Loss = 0.85761136, Test Loss = 0.86640924\n",
      "Epoch 46: Train Loss = 0.8687252, Validation Loss = 0.85026765, Test Loss = 0.8585632\n",
      "Epoch 47: Train Loss = 0.8616219, Validation Loss = 0.8428158, Test Loss = 0.850635\n",
      "Epoch 48: Train Loss = 0.8544436, Validation Loss = 0.8352301, Test Loss = 0.84262854\n",
      "Epoch 49: Train Loss = 0.84718984, Validation Loss = 0.8274893, Test Loss = 0.8345453\n",
      "Epoch 50: Train Loss = 0.83985984, Validation Loss = 0.81957775, Test Loss = 0.82638663\n",
      "Epoch 51: Train Loss = 0.83245444, Validation Loss = 0.8114883, Test Loss = 0.8181542\n",
      "Epoch 52: Train Loss = 0.8249756, Validation Loss = 0.80322254, Test Loss = 0.8098513\n",
      "Epoch 53: Train Loss = 0.8174282, Validation Loss = 0.7947906, Test Loss = 0.80148256\n",
      "Epoch 54: Train Loss = 0.8098189, Validation Loss = 0.7862108, Test Loss = 0.79305494\n",
      "Epoch 55: Train Loss = 0.8021561, Validation Loss = 0.7775068, Test Loss = 0.78457654\n",
      "Epoch 56: Train Loss = 0.79444885, Validation Loss = 0.7687075, Test Loss = 0.776056\n",
      "Epoch 57: Train Loss = 0.78670657, Validation Loss = 0.75984377, Test Loss = 0.76750207\n",
      "Epoch 58: Train Loss = 0.7789379, Validation Loss = 0.75094604, Test Loss = 0.75892323\n",
      "Epoch 59: Train Loss = 0.7711508, Validation Loss = 0.7420442, Test Loss = 0.750327\n",
      "Epoch 60: Train Loss = 0.76335245, Validation Loss = 0.7331652, Test Loss = 0.7417208\n",
      "Epoch 61: Train Loss = 0.7555493, Validation Loss = 0.7243326, Test Loss = 0.73311055\n",
      "Epoch 62: Train Loss = 0.7477475, Validation Loss = 0.71556586, Test Loss = 0.724503\n",
      "Epoch 63: Train Loss = 0.7399529, Validation Loss = 0.70688075, Test Loss = 0.71590465\n",
      "Epoch 64: Train Loss = 0.7321725, Validation Loss = 0.6982893, Test Loss = 0.7073228\n",
      "Epoch 65: Train Loss = 0.72441304, Validation Loss = 0.6897994, Test Loss = 0.698765\n",
      "Epoch 66: Train Loss = 0.7166824, Validation Loss = 0.681416, Test Loss = 0.69023955\n",
      "Epoch 67: Train Loss = 0.70898825, Validation Loss = 0.67314094, Test Loss = 0.6817554\n",
      "Epoch 68: Train Loss = 0.7013393, Validation Loss = 0.66497403, Test Loss = 0.67332166\n",
      "Epoch 69: Train Loss = 0.6937436, Validation Loss = 0.6569122, Test Loss = 0.664947\n",
      "Epoch 70: Train Loss = 0.6862092, Validation Loss = 0.64895225, Test Loss = 0.6566402\n",
      "Epoch 71: Train Loss = 0.67874336, Validation Loss = 0.6410892, Test Loss = 0.6484089\n",
      "Epoch 72: Train Loss = 0.671353, Validation Loss = 0.63331884, Test Loss = 0.64026046\n",
      "Epoch 73: Train Loss = 0.66404426, Validation Loss = 0.62563735, Test Loss = 0.63220125\n",
      "Epoch 74: Train Loss = 0.65682274, Validation Loss = 0.61804193, Test Loss = 0.62423724\n",
      "Epoch 75: Train Loss = 0.6496936, Validation Loss = 0.6105325, Test Loss = 0.6163736\n",
      "Epoch 76: Train Loss = 0.6426617, Validation Loss = 0.60310996, Test Loss = 0.6086151\n",
      "Epoch 77: Train Loss = 0.6357314, Validation Loss = 0.5957779, Test Loss = 0.6009663\n",
      "Epoch 78: Train Loss = 0.6289068, Validation Loss = 0.5885419, Test Loss = 0.5934313\n",
      "Epoch 79: Train Loss = 0.6221918, Validation Loss = 0.5814093, Test Loss = 0.5860136\n",
      "Epoch 80: Train Loss = 0.61558974, Validation Loss = 0.5743883, Test Loss = 0.57871693\n",
      "Epoch 81: Train Loss = 0.60910374, Validation Loss = 0.56748813, Test Loss = 0.57154393\n",
      "Epoch 82: Train Loss = 0.6027361, Validation Loss = 0.56071776, Test Loss = 0.56449693\n",
      "Epoch 83: Train Loss = 0.5964889, Validation Loss = 0.5540858, Test Loss = 0.55757767\n",
      "Epoch 84: Train Loss = 0.59036314, Validation Loss = 0.54759955, Test Loss = 0.55078727\n",
      "Epoch 85: Train Loss = 0.5843598, Validation Loss = 0.54126513, Test Loss = 0.5441265\n",
      "Epoch 86: Train Loss = 0.57847923, Validation Loss = 0.5350865, Test Loss = 0.53759557\n",
      "Epoch 87: Train Loss = 0.5727212, Validation Loss = 0.52906597, Test Loss = 0.5311942\n",
      "Epoch 88: Train Loss = 0.56708527, Validation Loss = 0.52320397, Test Loss = 0.5249217\n",
      "Epoch 89: Train Loss = 0.56157076, Validation Loss = 0.5174986, Test Loss = 0.5187773\n",
      "Epoch 90: Train Loss = 0.55617666, Validation Loss = 0.51194656, Test Loss = 0.5127598\n",
      "Epoch 91: Train Loss = 0.55090165, Validation Loss = 0.5065432, Test Loss = 0.50686765\n",
      "Epoch 92: Train Loss = 0.5457442, Validation Loss = 0.50128233, Test Loss = 0.50109935\n",
      "Epoch 93: Train Loss = 0.5407026, Validation Loss = 0.4961571, Test Loss = 0.4954527\n",
      "Epoch 94: Train Loss = 0.5357749, Validation Loss = 0.49116045, Test Loss = 0.48992556\n",
      "Epoch 95: Train Loss = 0.53095865, Validation Loss = 0.48628494, Test Loss = 0.4845155\n",
      "Epoch 96: Train Loss = 0.5262515, Validation Loss = 0.48152372, Test Loss = 0.4792198\n",
      "Epoch 97: Train Loss = 0.52165085, Validation Loss = 0.4768704, Test Loss = 0.47403568\n",
      "Epoch 98: Train Loss = 0.51715386, Validation Loss = 0.47231928, Test Loss = 0.4689601\n",
      "Epoch 99: Train Loss = 0.5127576, Validation Loss = 0.46786588, Test Loss = 0.46399006\n",
      "Epoch 100: Train Loss = 0.5084592, Validation Loss = 0.46350658, Test Loss = 0.45912245\n",
      "Epoch 0: Train Loss = 1.1168224, Validation Loss = 1.1146376, Test Loss = 1.1047796\n",
      "Epoch 1: Train Loss = 1.1124196, Validation Loss = 1.1125988, Test Loss = 1.1037678\n",
      "Epoch 2: Train Loss = 1.1088345, Validation Loss = 1.1113601, Test Loss = 1.1035477\n",
      "Epoch 3: Train Loss = 1.106066, Validation Loss = 1.1108849, Test Loss = 1.1040747\n",
      "Epoch 4: Train Loss = 1.1040772, Validation Loss = 1.1110853, Test Loss = 1.1052468\n",
      "Epoch 5: Train Loss = 1.1027855, Validation Loss = 1.1118106, Test Loss = 1.1069022\n",
      "Epoch 6: Train Loss = 1.1020527, Validation Loss = 1.1128465, Test Loss = 1.1088053\n",
      "Epoch 7: Train Loss = 1.10169, Validation Loss = 1.1139337, Test Loss = 1.110668\n",
      "Epoch 8: Train Loss = 1.1014928, Validation Loss = 1.1148201, Test Loss = 1.1122117\n",
      "Epoch 9: Train Loss = 1.1012875, Validation Loss = 1.1153182, Test Loss = 1.1132355\n",
      "Epoch 10: Train Loss = 1.1009618, Validation Loss = 1.1153327, Test Loss = 1.1136439\n",
      "Epoch 11: Train Loss = 1.1004688, Validation Loss = 1.114855, Test Loss = 1.1134368\n",
      "Epoch 12: Train Loss = 1.0998117, Validation Loss = 1.1139379, Test Loss = 1.1126798\n",
      "Epoch 13: Train Loss = 1.0990252, Validation Loss = 1.1126685, Test Loss = 1.1114746\n",
      "Epoch 14: Train Loss = 1.098159, Validation Loss = 1.1111495, Test Loss = 1.1099362\n",
      "Epoch 15: Train Loss = 1.0972648, Validation Loss = 1.1094842, Test Loss = 1.1081796\n",
      "Epoch 16: Train Loss = 1.0963883, Validation Loss = 1.1077688, Test Loss = 1.1063106\n",
      "Epoch 17: Train Loss = 1.0955645, Validation Loss = 1.1060852, Test Loss = 1.1044205\n",
      "Epoch 18: Train Loss = 1.0948136, Validation Loss = 1.1044992, Test Loss = 1.1025838\n",
      "Epoch 19: Train Loss = 1.0941408, Validation Loss = 1.1030562, Test Loss = 1.1008554\n",
      "Epoch 20: Train Loss = 1.0935376, Validation Loss = 1.101782, Test Loss = 1.0992728\n",
      "Epoch 21: Train Loss = 1.0929846, Validation Loss = 1.1006818, Test Loss = 1.0978557\n",
      "Epoch 22: Train Loss = 1.0924557, Validation Loss = 1.0997446, Test Loss = 1.0966097\n",
      "Epoch 23: Train Loss = 1.0919244, Validation Loss = 1.0989475, Test Loss = 1.0955303\n",
      "Epoch 24: Train Loss = 1.0913671, Validation Loss = 1.0982611, Test Loss = 1.0946058\n",
      "Epoch 25: Train Loss = 1.0907674, Validation Loss = 1.0976543, Test Loss = 1.0938212\n",
      "Epoch 26: Train Loss = 1.0901171, Validation Loss = 1.0970988, Test Loss = 1.0931602\n",
      "Epoch 27: Train Loss = 1.089416, Validation Loss = 1.0965695, Test Loss = 1.0926063\n",
      "Epoch 28: Train Loss = 1.08867, Validation Loss = 1.0960462, Test Loss = 1.0921432\n",
      "Epoch 29: Train Loss = 1.0878885, Validation Loss = 1.0955117, Test Loss = 1.0917516\n",
      "Epoch 30: Train Loss = 1.0870812, Validation Loss = 1.0949512, Test Loss = 1.0914116\n",
      "Epoch 31: Train Loss = 1.0862559, Validation Loss = 1.0943512, Test Loss = 1.0910983\n",
      "Epoch 32: Train Loss = 1.0854163, Validation Loss = 1.0936985, Test Loss = 1.0907844\n",
      "Epoch 33: Train Loss = 1.0845617, Validation Loss = 1.0929811, Test Loss = 1.0904388\n",
      "Epoch 34: Train Loss = 1.0836867, Validation Loss = 1.092187, Test Loss = 1.0900292\n",
      "Epoch 35: Train Loss = 1.0827821, Validation Loss = 1.0913066, Test Loss = 1.0895246\n",
      "Epoch 36: Train Loss = 1.0818366, Validation Loss = 1.090331, Test Loss = 1.0888957\n",
      "Epoch 37: Train Loss = 1.0808382, Validation Loss = 1.0892549, Test Loss = 1.0881201\n",
      "Epoch 38: Train Loss = 1.0797765, Validation Loss = 1.0880753, Test Loss = 1.0871818\n",
      "Epoch 39: Train Loss = 1.0786427, Validation Loss = 1.0867909, Test Loss = 1.0860728\n",
      "Epoch 40: Train Loss = 1.0774312, Validation Loss = 1.085403, Test Loss = 1.0847929\n",
      "Epoch 41: Train Loss = 1.076138, Validation Loss = 1.083913, Test Loss = 1.0833485\n",
      "Epoch 42: Train Loss = 1.0747609, Validation Loss = 1.0823216, Test Loss = 1.0817504\n",
      "Epoch 43: Train Loss = 1.0732977, Validation Loss = 1.0806292, Test Loss = 1.0800126\n",
      "Epoch 44: Train Loss = 1.0717462, Validation Loss = 1.0788332, Test Loss = 1.0781482\n",
      "Epoch 45: Train Loss = 1.070102, Validation Loss = 1.0769296, Test Loss = 1.0761703\n",
      "Epoch 46: Train Loss = 1.0683589, Validation Loss = 1.0749122, Test Loss = 1.0740882\n",
      "Epoch 47: Train Loss = 1.0665096, Validation Loss = 1.072773, Test Loss = 1.0719073\n",
      "Epoch 48: Train Loss = 1.064545, Validation Loss = 1.0705044, Test Loss = 1.0696297\n",
      "Epoch 49: Train Loss = 1.0624555, Validation Loss = 1.0680978, Test Loss = 1.0672532\n",
      "Epoch 50: Train Loss = 1.0602316, Validation Loss = 1.0655454, Test Loss = 1.0647722\n",
      "Epoch 51: Train Loss = 1.0578632, Validation Loss = 1.0628397, Test Loss = 1.0621784\n",
      "Epoch 52: Train Loss = 1.0553416, Validation Loss = 1.0599731, Test Loss = 1.0594599\n",
      "Epoch 53: Train Loss = 1.0526582, Validation Loss = 1.0569379, Test Loss = 1.0566037\n",
      "Epoch 54: Train Loss = 1.0498047, Validation Loss = 1.0537258, Test Loss = 1.0535939\n",
      "Epoch 55: Train Loss = 1.0467727, Validation Loss = 1.0503272, Test Loss = 1.0504146\n",
      "Epoch 56: Train Loss = 1.0435543, Validation Loss = 1.0467311, Test Loss = 1.0470475\n",
      "Epoch 57: Train Loss = 1.0401406, Validation Loss = 1.0429257, Test Loss = 1.0434753\n",
      "Epoch 58: Train Loss = 1.0365232, Validation Loss = 1.0388979, Test Loss = 1.0396796\n",
      "Epoch 59: Train Loss = 1.0326928, Validation Loss = 1.0346338, Test Loss = 1.0356439\n",
      "Epoch 60: Train Loss = 1.0286404, Validation Loss = 1.0301197, Test Loss = 1.0313513\n",
      "Epoch 61: Train Loss = 1.0243564, Validation Loss = 1.0253416, Test Loss = 1.0267873\n",
      "Epoch 62: Train Loss = 1.0198319, Validation Loss = 1.0202868, Test Loss = 1.021939\n",
      "Epoch 63: Train Loss = 1.0150576, Validation Loss = 1.0149432, Test Loss = 1.0167947\n",
      "Epoch 64: Train Loss = 1.0100249, Validation Loss = 1.0093004, Test Loss = 1.0113451\n",
      "Epoch 65: Train Loss = 1.004725, Validation Loss = 1.0033481, Test Loss = 1.0055819\n",
      "Epoch 66: Train Loss = 0.9991499, Validation Loss = 0.99707836, Test Loss = 0.9994984\n",
      "Epoch 67: Train Loss = 0.9932921, Validation Loss = 0.990484, Test Loss = 0.9930894\n",
      "Epoch 68: Train Loss = 0.98714465, Validation Loss = 0.98355883, Test Loss = 0.98634976\n",
      "Epoch 69: Train Loss = 0.9807014, Validation Loss = 0.97629744, Test Loss = 0.979276\n",
      "Epoch 70: Train Loss = 0.9739574, Validation Loss = 0.9686963, Test Loss = 0.97186506\n",
      "Epoch 71: Train Loss = 0.9669093, Validation Loss = 0.9607521, Test Loss = 0.96411484\n",
      "Epoch 72: Train Loss = 0.9595548, Validation Loss = 0.9524642, Test Loss = 0.95602477\n",
      "Epoch 73: Train Loss = 0.9518938, Validation Loss = 0.9438334, Test Loss = 0.9475958\n",
      "Epoch 74: Train Loss = 0.9439278, Validation Loss = 0.934863, Test Loss = 0.93883103\n",
      "Epoch 75: Train Loss = 0.9356603, Validation Loss = 0.92555803, Test Loss = 0.9297348\n",
      "Epoch 76: Train Loss = 0.92709726, Validation Loss = 0.91592693, Test Loss = 0.9203149\n",
      "Epoch 77: Train Loss = 0.91824627, Validation Loss = 0.90598017, Test Loss = 0.9105807\n",
      "Epoch 78: Train Loss = 0.9091178, Validation Loss = 0.8957314, Test Loss = 0.90054435\n",
      "Epoch 79: Train Loss = 0.8997243, Validation Loss = 0.8851967, Test Loss = 0.8902205\n",
      "Epoch 80: Train Loss = 0.8900806, Validation Loss = 0.8743948, Test Loss = 0.87962633\n",
      "Epoch 81: Train Loss = 0.8802038, Validation Loss = 0.86334735, Test Loss = 0.86878186\n",
      "Epoch 82: Train Loss = 0.8701132, Validation Loss = 0.8520783, Test Loss = 0.85770905\n",
      "Epoch 83: Train Loss = 0.8598302, Validation Loss = 0.8406139, Test Loss = 0.84643215\n",
      "Epoch 84: Train Loss = 0.8493783, Validation Loss = 0.82898253, Test Loss = 0.8349781\n",
      "Epoch 85: Train Loss = 0.8387825, Validation Loss = 0.81721455, Test Loss = 0.82337487\n",
      "Epoch 86: Train Loss = 0.82806957, Validation Loss = 0.8053412, Test Loss = 0.8116528\n",
      "Epoch 87: Train Loss = 0.8172674, Validation Loss = 0.793396, Test Loss = 0.79984283\n",
      "Epoch 88: Train Loss = 0.80640495, Validation Loss = 0.78141224, Test Loss = 0.78797704\n",
      "Epoch 89: Train Loss = 0.7955119, Validation Loss = 0.769424, Test Loss = 0.77608836\n",
      "Epoch 90: Train Loss = 0.7846178, Validation Loss = 0.7574653, Test Loss = 0.76420945\n",
      "Epoch 91: Train Loss = 0.77375287, Validation Loss = 0.7455695, Test Loss = 0.75237304\n",
      "Epoch 92: Train Loss = 0.7629463, Validation Loss = 0.7337696, Test Loss = 0.74061114\n",
      "Epoch 93: Train Loss = 0.752227, Validation Loss = 0.72209656, Test Loss = 0.7289547\n",
      "Epoch 94: Train Loss = 0.74162275, Validation Loss = 0.7105805, Test Loss = 0.7174333\n",
      "Epoch 95: Train Loss = 0.73115987, Validation Loss = 0.6992492, Test Loss = 0.7060749\n",
      "Epoch 96: Train Loss = 0.7208633, Validation Loss = 0.68812823, Test Loss = 0.6949056\n",
      "Epoch 97: Train Loss = 0.7107558, Validation Loss = 0.6772411, Test Loss = 0.68394905\n",
      "Epoch 98: Train Loss = 0.70085865, Validation Loss = 0.6666085, Test Loss = 0.67322695\n",
      "Epoch 99: Train Loss = 0.6911905, Validation Loss = 0.6562486, Test Loss = 0.6627581\n",
      "Epoch 100: Train Loss = 0.68176806, Validation Loss = 0.64617705, Test Loss = 0.6525594\n",
      "Epoch 0: Train Loss = 1.5726587, Validation Loss = 1.3882884, Test Loss = 1.4890192\n",
      "Epoch 1: Train Loss = 1.5352497, Validation Loss = 1.358085, Test Loss = 1.4567882\n",
      "Epoch 2: Train Loss = 1.5001254, Validation Loss = 1.3300564, Test Loss = 1.4267163\n",
      "Epoch 3: Train Loss = 1.4672767, Validation Loss = 1.3041973, Test Loss = 1.398772\n",
      "Epoch 4: Train Loss = 1.4366553, Validation Loss = 1.2804701, Test Loss = 1.3728786\n",
      "Epoch 5: Train Loss = 1.4081687, Validation Loss = 1.2587985, Test Loss = 1.3489109\n",
      "Epoch 6: Train Loss = 1.3816781, Validation Loss = 1.239065, Test Loss = 1.326697\n",
      "Epoch 7: Train Loss = 1.3570123, Validation Loss = 1.2211165, Test Loss = 1.3060375\n",
      "Epoch 8: Train Loss = 1.3339936, Validation Loss = 1.2047828, Test Loss = 1.2867404\n",
      "Epoch 9: Train Loss = 1.3124647, Validation Loss = 1.1899025, Test Loss = 1.2686492\n",
      "Epoch 10: Train Loss = 1.2923043, Validation Loss = 1.1763383, Test Loss = 1.251657\n",
      "Epoch 11: Train Loss = 1.2734277, Validation Loss = 1.1639837, Test Loss = 1.2357001\n",
      "Epoch 12: Train Loss = 1.2557776, Validation Loss = 1.1527574, Test Loss = 1.2207458\n",
      "Epoch 13: Train Loss = 1.2393138, Validation Loss = 1.1425972, Test Loss = 1.2067776\n",
      "Epoch 14: Train Loss = 1.224005, Validation Loss = 1.133452, Test Loss = 1.1937841\n",
      "Epoch 15: Train Loss = 1.2098219, Validation Loss = 1.1252764, Test Loss = 1.1817545\n",
      "Epoch 16: Train Loss = 1.196735, Validation Loss = 1.1180265, Test Loss = 1.1706737\n",
      "Epoch 17: Train Loss = 1.1847113, Validation Loss = 1.111658, Test Loss = 1.1605217\n",
      "Epoch 18: Train Loss = 1.173714, Validation Loss = 1.1061249, Test Loss = 1.1512731\n",
      "Epoch 19: Train Loss = 1.1637015, Validation Loss = 1.1013781, Test Loss = 1.1428968\n",
      "Epoch 20: Train Loss = 1.1546284, Validation Loss = 1.0973667, Test Loss = 1.1353558\n",
      "Epoch 21: Train Loss = 1.1464452, Validation Loss = 1.0940369, Test Loss = 1.1286104\n",
      "Epoch 22: Train Loss = 1.1390991, Validation Loss = 1.0913335, Test Loss = 1.1226145\n",
      "Epoch 23: Train Loss = 1.1325353, Validation Loss = 1.0891995, Test Loss = 1.1173213\n",
      "Epoch 24: Train Loss = 1.1266971, Validation Loss = 1.0875776, Test Loss = 1.1126806\n",
      "Epoch 25: Train Loss = 1.121527, Validation Loss = 1.0864109, Test Loss = 1.1086411\n",
      "Epoch 26: Train Loss = 1.1169674, Validation Loss = 1.085643, Test Loss = 1.1051519\n",
      "Epoch 27: Train Loss = 1.1129621, Validation Loss = 1.0852195, Test Loss = 1.1021609\n",
      "Epoch 28: Train Loss = 1.1094555, Validation Loss = 1.0850874, Test Loss = 1.0996187\n",
      "Epoch 29: Train Loss = 1.1063943, Validation Loss = 1.0851976, Test Loss = 1.0974762\n",
      "Epoch 30: Train Loss = 1.1037282, Validation Loss = 1.0855029, Test Loss = 1.0956872\n",
      "Epoch 31: Train Loss = 1.1014094, Validation Loss = 1.0859604, Test Loss = 1.0942074\n",
      "Epoch 32: Train Loss = 1.0993936, Validation Loss = 1.0865307, Test Loss = 1.0929959\n",
      "Epoch 33: Train Loss = 1.0976399, Validation Loss = 1.0871785, Test Loss = 1.0920141\n",
      "Epoch 34: Train Loss = 1.0961109, Validation Loss = 1.0878723, Test Loss = 1.0912277\n",
      "Epoch 35: Train Loss = 1.0947734, Validation Loss = 1.0885847, Test Loss = 1.0906042\n",
      "Epoch 36: Train Loss = 1.0935968, Validation Loss = 1.0892922, Test Loss = 1.0901153\n",
      "Epoch 37: Train Loss = 1.092555, Validation Loss = 1.089975, Test Loss = 1.0897355\n",
      "Epoch 38: Train Loss = 1.0916247, Validation Loss = 1.0906167, Test Loss = 1.089442\n",
      "Epoch 39: Train Loss = 1.0907859, Validation Loss = 1.0912043, Test Loss = 1.0892152\n",
      "Epoch 40: Train Loss = 1.0900208, Validation Loss = 1.0917274, Test Loss = 1.0890373\n",
      "Epoch 41: Train Loss = 1.0893148, Validation Loss = 1.0921781, Test Loss = 1.0888935\n",
      "Epoch 42: Train Loss = 1.0886552, Validation Loss = 1.092551, Test Loss = 1.0887707\n",
      "Epoch 43: Train Loss = 1.0880313, Validation Loss = 1.0928422, Test Loss = 1.0886575\n",
      "Epoch 44: Train Loss = 1.0874339, Validation Loss = 1.0930499, Test Loss = 1.0885442\n",
      "Epoch 45: Train Loss = 1.0868558, Validation Loss = 1.0931729, Test Loss = 1.0884224\n",
      "Epoch 46: Train Loss = 1.08629, Validation Loss = 1.0932117, Test Loss = 1.088285\n",
      "Epoch 47: Train Loss = 1.0857314, Validation Loss = 1.0931668, Test Loss = 1.0881252\n",
      "Epoch 48: Train Loss = 1.085175, Validation Loss = 1.0930402, Test Loss = 1.087938\n",
      "Epoch 0: Train Loss = 1.2090563, Validation Loss = 1.3054512, Test Loss = 1.2354692\n",
      "Epoch 1: Train Loss = 1.1456734, Validation Loss = 1.2141209, Test Loss = 1.1647637\n",
      "Epoch 2: Train Loss = 1.1079736, Validation Loss = 1.149928, Test Loss = 1.1202886\n",
      "Epoch 3: Train Loss = 1.092272, Validation Loss = 1.1101685, Test Loss = 1.0985752\n",
      "Epoch 4: Train Loss = 1.0919186, Validation Loss = 1.0895032, Test Loss = 1.0932966\n",
      "Epoch 5: Train Loss = 1.0986986, Validation Loss = 1.0809036, Test Loss = 1.0964967\n",
      "Epoch 6: Train Loss = 1.1056087, Validation Loss = 1.0778553, Test Loss = 1.1012048\n",
      "Epoch 7: Train Loss = 1.1087947, Validation Loss = 1.076162, Test Loss = 1.1033992\n",
      "Epoch 8: Train Loss = 1.1073416, Validation Loss = 1.0740991, Test Loss = 1.1019213\n",
      "Epoch 9: Train Loss = 1.1020573, Validation Loss = 1.0715631, Test Loss = 1.0973469\n",
      "Epoch 10: Train Loss = 1.094452, Validation Loss = 1.0692137, Test Loss = 1.0909868\n",
      "Epoch 11: Train Loss = 1.0861439, Validation Loss = 1.0679084, Test Loss = 1.0842898\n",
      "Epoch 12: Train Loss = 1.0785265, Validation Loss = 1.0683459, Test Loss = 1.0784951\n",
      "Epoch 13: Train Loss = 1.0725563, Validation Loss = 1.0708256, Test Loss = 1.0744117\n",
      "Epoch 14: Train Loss = 1.0686119, Validation Loss = 1.075091, Test Loss = 1.07227\n",
      "Epoch 15: Train Loss = 1.0664513, Validation Loss = 1.0803002, Test Loss = 1.0716846\n",
      "Epoch 16: Train Loss = 1.0653212, Validation Loss = 1.0851746, Test Loss = 1.0717741\n",
      "Epoch 17: Train Loss = 1.0642235, Validation Loss = 1.0883242, Test Loss = 1.0714469\n",
      "Epoch 18: Train Loss = 1.0622597, Validation Loss = 1.0886369, Test Loss = 1.0697559\n",
      "Epoch 19: Train Loss = 1.0588986, Validation Loss = 1.0855616, Test Loss = 1.0661701\n",
      "Epoch 20: Train Loss = 1.0540696, Validation Loss = 1.079179, Test Loss = 1.0606611\n",
      "Epoch 21: Train Loss = 1.0480788, Validation Loss = 1.0700743, Test Loss = 1.0536069\n",
      "Epoch 22: Train Loss = 1.0414239, Validation Loss = 1.0591087, Test Loss = 1.0455966\n",
      "Epoch 23: Train Loss = 1.0345982, Validation Loss = 1.0471798, Test Loss = 1.0372229\n",
      "Epoch 24: Train Loss = 1.0279398, Validation Loss = 1.0350405, Test Loss = 1.028922\n",
      "Epoch 25: Train Loss = 1.0215575, Validation Loss = 1.0231954, Test Loss = 1.020894\n",
      "Epoch 26: Train Loss = 1.0153382, Validation Loss = 1.011882, Test Loss = 1.0131032\n",
      "Epoch 27: Train Loss = 1.0090144, Validation Loss = 1.0011178, Test Loss = 1.0053405\n",
      "Epoch 28: Train Loss = 1.0022624, Validation Loss = 0.99078095, Test Loss = 0.99732053\n",
      "Epoch 29: Train Loss = 0.99479777, Validation Loss = 0.9806983, Test Loss = 0.98877597\n",
      "Epoch 30: Train Loss = 0.9864397, Validation Loss = 0.97070885, Test Loss = 0.9795234\n",
      "Epoch 31: Train Loss = 0.97713405, Validation Loss = 0.9606961, Test Loss = 0.96948916\n",
      "Epoch 32: Train Loss = 0.96693844, Validation Loss = 0.95058453, Test Loss = 0.95869637\n",
      "Epoch 33: Train Loss = 0.95598006, Validation Loss = 0.9403139, Test Loss = 0.94722545\n",
      "Epoch 34: Train Loss = 0.94440246, Validation Loss = 0.92980283, Test Loss = 0.93516576\n",
      "Epoch 35: Train Loss = 0.932318, Validation Loss = 0.91892034, Test Loss = 0.9225706\n",
      "Epoch 36: Train Loss = 0.91977525, Validation Loss = 0.90747297, Test Loss = 0.9094319\n",
      "Epoch 37: Train Loss = 0.9067551, Validation Loss = 0.8952198, Test Loss = 0.8956789\n",
      "Epoch 38: Train Loss = 0.8931875, Validation Loss = 0.8819084, Test Loss = 0.8812025\n",
      "Epoch 39: Train Loss = 0.87898916, Validation Loss = 0.86732763, Test Loss = 0.86589503\n",
      "Epoch 40: Train Loss = 0.8641022, Validation Loss = 0.85135573, Test Loss = 0.8496924\n",
      "Epoch 41: Train Loss = 0.8485234, Validation Loss = 0.83399284, Test Loss = 0.8326038\n",
      "Epoch 42: Train Loss = 0.8323137, Validation Loss = 0.8153643, Test Loss = 0.8147175\n",
      "Epoch 43: Train Loss = 0.81558675, Validation Loss = 0.79570025, Test Loss = 0.7961864\n",
      "Epoch 44: Train Loss = 0.7984848, Validation Loss = 0.7752957, Test Loss = 0.7771985\n",
      "Epoch 45: Train Loss = 0.78115016, Validation Loss = 0.75446737, Test Loss = 0.7579429\n",
      "Epoch 46: Train Loss = 0.76370454, Validation Loss = 0.7335153, Test Loss = 0.7385833\n",
      "Epoch 47: Train Loss = 0.74623954, Validation Loss = 0.71269906, Test Loss = 0.719245\n",
      "Epoch 48: Train Loss = 0.72882015, Validation Loss = 0.6922273, Test Loss = 0.70001525\n",
      "Epoch 49: Train Loss = 0.7114957, Validation Loss = 0.6722606, Test Loss = 0.6809545\n",
      "Epoch 50: Train Loss = 0.69431543, Validation Loss = 0.652919, Test Loss = 0.6621118\n",
      "Epoch 51: Train Loss = 0.677339, Validation Loss = 0.6342914, Test Loss = 0.6435394\n",
      "Epoch 52: Train Loss = 0.6606419, Validation Loss = 0.6164408, Test Loss = 0.6253\n",
      "Epoch 53: Train Loss = 0.6443126, Validation Loss = 0.5994069, Test Loss = 0.60746706\n",
      "Epoch 54: Train Loss = 0.628444, Validation Loss = 0.58320343, Test Loss = 0.59011936\n",
      "Epoch 55: Train Loss = 0.61312115, Validation Loss = 0.5678157, Test Loss = 0.5733302\n",
      "Epoch 56: Train Loss = 0.59841096, Validation Loss = 0.5532009, Test Loss = 0.55715793\n",
      "Epoch 57: Train Loss = 0.58435524, Validation Loss = 0.53929204, Test Loss = 0.54164\n",
      "Epoch 58: Train Loss = 0.5709703, Validation Loss = 0.5260081, Test Loss = 0.5267909\n",
      "Epoch 59: Train Loss = 0.55825174, Validation Loss = 0.5132684, Test Loss = 0.51260746\n",
      "Epoch 60: Train Loss = 0.5461826, Validation Loss = 0.50100595, Test Loss = 0.49907428\n",
      "Epoch 61: Train Loss = 0.5347408, Validation Loss = 0.48917872, Test Loss = 0.4861706\n",
      "Epoch 62: Train Loss = 0.5239037, Validation Loss = 0.47777307, Test Loss = 0.47387427\n",
      "Epoch 63: Train Loss = 0.51364875, Validation Loss = 0.46680033, Test Loss = 0.46216166\n",
      "Epoch 64: Train Loss = 0.50395066, Validation Loss = 0.45628792, Test Loss = 0.4510058\n",
      "Epoch 65: Train Loss = 0.4947793, Validation Loss = 0.4462671, Test Loss = 0.44037408\n",
      "Epoch 66: Train Loss = 0.48609722, Validation Loss = 0.4367627, Test Loss = 0.43022746\n",
      "Epoch 67: Train Loss = 0.47786212, Validation Loss = 0.42778537, Test Loss = 0.42052284\n",
      "Epoch 68: Train Loss = 0.47002935, Validation Loss = 0.41932774, Test Loss = 0.41121593\n",
      "Epoch 69: Train Loss = 0.46255612, Validation Loss = 0.4113644, Test Loss = 0.40226597\n",
      "Epoch 70: Train Loss = 0.4554041, Validation Loss = 0.40385273, Test Loss = 0.39363727\n",
      "Epoch 71: Train Loss = 0.4485399, Validation Loss = 0.39673677, Test Loss = 0.38530058\n",
      "Epoch 72: Train Loss = 0.44193438, Validation Loss = 0.38995042, Test Loss = 0.37723085\n",
      "Epoch 73: Train Loss = 0.43556085, Validation Loss = 0.38342354, Test Loss = 0.36940584\n",
      "Epoch 74: Train Loss = 0.42939466, Validation Loss = 0.37708795, Test Loss = 0.3618039\n",
      "Epoch 75: Train Loss = 0.423413, Validation Loss = 0.3708847, Test Loss = 0.354403\n",
      "Epoch 76: Train Loss = 0.41759583, Validation Loss = 0.36476985, Test Loss = 0.34718162\n",
      "Epoch 77: Train Loss = 0.41192573, Validation Loss = 0.358717, Test Loss = 0.34011802\n",
      "Epoch 78: Train Loss = 0.406388, Validation Loss = 0.35271677, Test Loss = 0.3331913\n",
      "Epoch 79: Train Loss = 0.40096974, Validation Loss = 0.34677264, Test Loss = 0.3263816\n",
      "Epoch 80: Train Loss = 0.39565942, Validation Loss = 0.34089604, Test Loss = 0.31967172\n",
      "Epoch 81: Train Loss = 0.390447, Validation Loss = 0.33509997, Test Loss = 0.31304842\n",
      "Epoch 82: Train Loss = 0.38532418, Validation Loss = 0.32939446, Test Loss = 0.3065042\n",
      "Epoch 83: Train Loss = 0.38028494, Validation Loss = 0.32378343, Test Loss = 0.3000377\n",
      "Epoch 84: Train Loss = 0.37532508, Validation Loss = 0.31826258, Test Loss = 0.29365328\n",
      "Epoch 85: Train Loss = 0.37044147, Validation Loss = 0.31282037, Test Loss = 0.28735867\n",
      "Epoch 86: Train Loss = 0.36563098, Validation Loss = 0.3074413, Test Loss = 0.28116313\n",
      "Epoch 87: Train Loss = 0.36088997, Validation Loss = 0.30210963, Test Loss = 0.2750742\n",
      "Epoch 88: Train Loss = 0.35621503, Validation Loss = 0.29681617, Test Loss = 0.26909763\n",
      "Epoch 89: Train Loss = 0.35160303, Validation Loss = 0.2915619, Test Loss = 0.26323512\n",
      "Epoch 90: Train Loss = 0.34705186, Validation Loss = 0.28635964, Test Loss = 0.25748563\n",
      "Epoch 91: Train Loss = 0.34255973, Validation Loss = 0.28123125, Test Loss = 0.2518451\n",
      "Epoch 92: Train Loss = 0.33812463, Validation Loss = 0.2762017, Test Loss = 0.24630722\n",
      "Epoch 93: Train Loss = 0.33374417, Validation Loss = 0.27129188, Test Loss = 0.24086595\n",
      "Epoch 94: Train Loss = 0.3294159, Validation Loss = 0.2665127, Test Loss = 0.23551682\n",
      "Epoch 95: Train Loss = 0.32513762, Validation Loss = 0.2618613, Test Loss = 0.23025839\n",
      "Epoch 96: Train Loss = 0.32090738, Validation Loss = 0.2573218, Test Loss = 0.22509246\n",
      "Epoch 97: Train Loss = 0.3167232, Validation Loss = 0.25286922, Test Loss = 0.22002217\n",
      "Epoch 98: Train Loss = 0.31258264, Validation Loss = 0.24847683, Test Loss = 0.21505079\n",
      "Epoch 99: Train Loss = 0.3084833, Validation Loss = 0.24412446, Test Loss = 0.21017952\n",
      "Epoch 100: Train Loss = 0.30442291, Validation Loss = 0.23980516, Test Loss = 0.20540692\n"
     ]
    }
   ],
   "source": [
    "#Develop here the corresponding code and/or in an external file\n",
    "#You can load instead of copy and paste\n",
    "\n",
    "include(\"functions.jl\");\n",
    "\n",
    "using DelimitedFiles;\n",
    "\n",
    "#Define the parameters\n",
    "topologies = [[8], [4, 3], [6, 4], [16, 16]]; \n",
    "learningRate = 0.01;\n",
    "numMaxEpochs = 100; \n",
    "\n",
    "# Load the dataset\n",
    "dataset = readdlm(\"iris.data\",',');\n",
    "# Prepare the data\n",
    "inputs = convert(Array{Float32,2}, dataset[:,1:4]);\n",
    "normalizeMinMax!(inputs)\n",
    "targets = oneHotEncoding(dataset[:,5]);\n",
    "\n",
    "(trainIndex, validationIndex, testIndex) = holdOut(size(inputs, 1), 0.2, 0.2)\n",
    "\n",
    "trainingInputs = inputs[trainIndex, :]\n",
    "testInputs = inputs[testIndex, :]\n",
    "trainingTargets = targets[trainIndex, :]\n",
    "testTargets = targets[testIndex, :]\n",
    "validationInputs = inputs[validationIndex, :]\n",
    "validationTargets = targets[validationIndex, :]\n",
    "\n",
    "results = Float32[]\n",
    "\n",
    "for topology in topologies\n",
    "    \n",
    "    _, _, _, testLosses = trainClassANN(topology, (trainingInputs, trainingTargets);\n",
    "        validationDataset = (validationInputs, validationTargets),\n",
    "        testDataset = (testInputs, testTargets),\n",
    "        maxEpochs = numMaxEpochs, learningRate = learningRate, showText=true)\n",
    "\n",
    "    push!(results, testLosses[end])\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e4832",
   "metadata": {},
   "source": [
    "### Question\n",
    "Has the test error always decreased or has there come a point where it has started to increase?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046dd2ea",
   "metadata": {},
   "source": [
    "`Answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea20d7",
   "metadata": {},
   "source": [
    "### Question\n",
    "How do the 3 precision values evolve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8458c651",
   "metadata": {},
   "source": [
    "`Answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd81f5a8",
   "metadata": {},
   "source": [
    "### Question\n",
    "Which criteria usually stop the training? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c4036",
   "metadata": {},
   "source": [
    "`Answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2222a0",
   "metadata": {},
   "source": [
    "### Question\n",
    "To which cycle does the ANN returned by the function correspond? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16dc42e",
   "metadata": {},
   "source": [
    "`Answer here`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79d3366",
   "metadata": {},
   "source": [
    "# Julia Notes\n",
    "\n",
    "Julia has a library that allows you to display all kinds of plots in a very simple way.  To load it, simply put `using Plots`, the documentation can be found at http://docs.juliaplots.org/. As always the first should be to install, ifg it is not already installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b94bf546",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/opt/julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/opt/julia/environments/v1.9/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg; Pkg.add(\"Plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528b473",
   "metadata": {},
   "source": [
    "Actually, `Plots.jl` is not a package for displaying plots, but an interface to a set of libraries for displaying plots.  Plots.jl provides a set of functions that allow, in a unified way, to display plots using one or another library, with the same calls.  That is, what Plots.jl does is to interpret the commands and generate the plots using another graphics library, which it refers to as a backend. Therefore, it is possible to change the graphics library (backend) without having to modify the code, since the calls to the corresponding functions are the same for all backends. \n",
    "\n",
    "So, first of all, after loading `Plots`, is to select the backend you want to work with.  Julia provides a large number of widely used ones, which you have to install in the usual way if you want to use them. If you don't indicate which one you want to use, Julia defaults to choosing one depending on which ones are installed. To see which backend is being used, you can simply type `backend()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1542555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plots.GRBackend()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "backend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88893360",
   "metadata": {},
   "source": [
    "Some of the most common backends are: Plotly, PyPlot, PlotlyJS and GR. In general, to use a backend, you simply make a call to a function named after the backend itself, but in lowercase. For the 4 given as examples at the beginning of this paragraph, it would be with calls to `plotly()`, `pyplot()`, `plotlyjs()` and `gr()` respectively. As said before, in case it is not installed you will have to install it in the usual way, writing `Pkg.add(\"Plotly\")`, `Pkg.add(\"PyPlot\")`, `Pkg.add(\"PlotlyJS\")` or `Pkg.add(\"GR\")` respectively.   More information about the different backends can be found at https://docs.juliaplots.org/latest/backends/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1910b98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/opt/julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/opt/julia/environments/v1.9/Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/opt/julia/environments/v1.9/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `/opt/julia/environments/v1.9/Manifest.toml`\n"
     ]
    },
    {
     "data": {
      "application/vnd.webio.node+json": {
       "children": [],
       "instanceArgs": {
        "namespace": "html",
        "tag": "div"
       },
       "nodeType": "DOM",
       "props": {},
       "type": "node"
      },
      "text/html": [
       "<div style=\"padding: 1em; background-color: #f8d6da; border: 1px solid #f5c6cb; font-weight: bold;\">\n",
       "<p>The WebIO Jupyter extension was not detected. See the\n",
       "<a href=\"https://juliagizmos.github.io/WebIO.jl/latest/providers/ijulia/\" target=\"_blank\">\n",
       "    WebIO Jupyter integration documentation\n",
       "</a>\n",
       "for more information.\n",
       "</div>\n"
      ],
      "text/plain": [
       "WebIO._IJuliaInit()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Plots.PyPlotBackend()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install the pyplot and plotly if you do not have it\n",
    "using Pkg;\n",
    "Pkg.add(\"PyPlot\")\n",
    "Pkg.add(\"PlotlyJS\")\n",
    "plotlyjs()\n",
    "pyplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569e18a4",
   "metadata": {},
   "source": [
    "Once the desired backend is loaded, or with the default backend, you can start plotting. The easiest way is to use the plot function, which receives two parameters: the series to be put on the x-axis, and the series to be put on the y-axis, both as vectors, for example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f468771f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:5, [2, 3, 1, 3, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702400b8",
   "metadata": {},
   "source": [
    "In general, if you want to put more than one series on the graph, it is sufficient to specify one column per series in the matrix you pass to it for the y-axis data, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71af884",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:5, [2 1; 3 -1; 1 0; 3 2; 5 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb10562",
   "metadata": {},
   "source": [
    "Another way to do this is to add more series to the plot object that has been created. That is, the call to plot returns an object that can be modified by further calls to `plot!` and passing it as the first parameter (remember that when the name of a function ends in `!` the argument passed is modified). The above example could be done in the following way: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot(1:5, [2, 3, 1, 3, 5]) \n",
    "plot!(g, 1:5, [1, -1, 0, 2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b88d0",
   "metadata": {},
   "source": [
    "Or alternatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb9829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = plot() \n",
    "plot!(g, 1:5, [2, 3, 1, 3, 5]) \n",
    "plot!(g, 1:5, [1, -1, 0, 2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f697e3d6",
   "metadata": {},
   "source": [
    "For any of these three possibilities, if this is done in the command interpreter, the graph will appear.  In fact, in the second and third possibilities, more than one graph will appear as you create them. However, if this is done in a script, the graph does not automatically appear.  If we want it to appear, we have to make a call to display in this way: \n",
    "\n",
    "```julia\n",
    "    display(g)\n",
    "```\n",
    "\n",
    "Besides the function plot, there are many other functions that allow you to represent other types of graphs and charts with the usual names, such as `heatmap`, `plot3d`, `scatter`, `histogram`, `boxplot`, `violin`, etc., all of them with their corresponding function ending in `!`.  As always, you can consult the help of a function by typing, for example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dab179",
   "metadata": {},
   "outputs": [],
   "source": [
    "?boxplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e325c",
   "metadata": {},
   "source": [
    "Generally speaking, it is desired to add more information to the graphs than just the data used in the graph.  This information usually includes title, labels on the axes, legend, markers at each point, line type, colours, etc.  This is usually done through attributes passed as keywords in function calls, such as `axis`, `label`, `line`, `fill`, `marker`, `ticks`, `title`, `xlabel`, `ylabel`, for example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dedc8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:5, [2, 3, 1, 3, 5], \n",
    "    xaxis = \"Epoch\", yaxis = \"Loss value\", title = \"An plot example\", \n",
    "    marker = :square, color = :red, label = \"Serie 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81203ef",
   "metadata": {},
   "source": [
    "A quick overview of the most common keywords can be found at https://docs.juliaplots.org/latest/attributes/.   On the other hand, https://docs.juliaplots.org/latest/generated/supported/tenéis has a complete list of the different types of series, keywords, markers, line styles and scales supported by each backend.  As you can see, the list of graphs and arguments is quite extensive, which gives you an idea of the enormous possibilities Julia offers when it comes to plotting graphs.\n",
    "\n",
    "Another common issue when displaying graphs is to combine several graphs within a single window.  There are several methods to do this, the simplest and most commonly used being by using the keyword `layout` in the call to the corresponding function to generate the graph containing all of them.  In this sense, it is the easiest way to generate the graphs independently, and combine them later in a new call using the keyword `layout`, which receives as parameter a tuple with the number of rows and columns of the matrix of graphs, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352fe7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = plot(1:4, [1, 3, 4, 2]); \n",
    "p2 = plot(1:4, [2, 3, 2, 1]); \n",
    "p3 = plot(1:4, [3, 2, 1, 2]); \n",
    "plot(p1, p2, p3, layout = (3,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f7afea",
   "metadata": {},
   "source": [
    "As you could see, the plots asre arrange in a column layout each one in a row, alternatively, if we invert the order the result could be set in a single row as in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f386e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(p1, p2, p3, layout = (1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15abf21",
   "metadata": {},
   "source": [
    "More information on how to combine graphics can be found at https://docs.juliaplots.org/latest/layouts/. \n",
    "\n",
    "Finally, another very common action is to save the generated graphs.  This can be done by means of the savefig function, which receives as parameters the graph to save and the file name. From the extension indicated in the file name, Julia saves the graph in the indicated format. Some of the most typical formats are `pdf`, `png` or `ps`.  Although this is a very easy way to save the graphs, not all file types are supported by all backends. At https://docs.juliaplots.org/latest/output/ you can check the file formats supported by each backend. For exple with the previous example.\n",
    "\n",
    "```julia\n",
    "    g = plot(p1, p2, p3, layout = (1,3))\n",
    "    savefig(g, 'ExampleSingleRow.png')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698d78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
